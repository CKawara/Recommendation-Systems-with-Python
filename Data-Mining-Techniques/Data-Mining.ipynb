{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques, Principles and Algorithms that go into building collaborative filters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures\n",
    "How can we mathematically quantify how different or similar two items are to each other\n",
    "\n",
    " 1. Euclidean distance - the length of the line segment joining two data points plotted on an n-dimensional Cartesian plane **The lower the Euclidean score, the more similar the two points are to each other**\n",
    "\n",
    "The Euclidean score is mathematically defined as:\n",
    "\n",
    "$$\n",
    "  d(\\mathbf {v1,v2})= \\sqrt{\\sum \\limits_{i=1}^n (q_i-r_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute Euclidean distance\n",
    "def euclidean(v1, v2):\n",
    "    # convert 1-D Python lists to numpy vectors\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    # compute vectors which is the element wise square of the distance\n",
    "    diff = np.power(np.array(v1) - np.array(v2), 2)\n",
    "    # perform summation of the elements of the above vector\n",
    "    sigma_val = np.sum(diff)\n",
    "    # compute square root amd return final euclidean score\n",
    "    euclid_score =np.sqrt(sigma_val)\n",
    "    return euclid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.483314773547883"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define 3 users with rating for 5 movies\n",
    "u1 = [5,1,2,4,5]\n",
    "u2 = [1,5,4,2,1]\n",
    "u3 = [5,2,2,4,4]\n",
    "\n",
    "# from this, we see that 1 and 3 have fairly similar taste, while 2 and 1 have totally different tastes. Lets test5 our funtion \n",
    "euclidean(u1, u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(u1, u3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 and 3 have a smaller euclidean score between them, compared to the score between 1 and 2. \n",
    "This proves that 1 and 3 have similar taste and the euclidean distance was able to capture the relationship between our users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. #### Pearson correlation\n",
    "\n",
    "This is a score between -1 and 1, where -1 indicates total negative correlation and 1 indicates total positive correlation. 0 indicates that the two entities are in no way correlated with each other(independent of each other)\n",
    "\n",
    "Take for example movie ratings from 2 users Bob and Alice\n",
    "```\n",
    " Alice = [1,1,3,2,4]\n",
    " Bob = [2,2,4,3,5]\n",
    " ```\n",
    "Their Eucledean distance would be ``` 2.2360679774997898 ```\n",
    "\n",
    "But by just looking you can tell that Bob's rating is always higher by one compared to Alice's ratings. So it is safe to say that they are correlated.\n",
    "\n",
    "Consider anothe user, Eve\n",
    "```\n",
    "eve = [5,5,3,4,2]\n",
    "```\n",
    "We see that eve has a totally different taste from Alice and their euclidean distance, ```6.324555320336759```\n",
    "\n",
    "With Pearson correlation, we are able to make predictions on two very different people, as long as we have the rating of one, something euclidean distance can't\n",
    "\n",
    "Pearson correlation can be expressed mathematically as follows:\n",
    "$$\n",
    "r =\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum\\left(x_{i}-\\bar{x}\\right)^{2} \\sum\\left(y_{i}-\\bar{y}\\right)^{2}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=1.0, pvalue=0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SciPy gives us access to a function that computes the pearson similarity scores\n",
    "from scipy.stats import pearsonr\n",
    "Alice = [1,1,3,2,4]\n",
    "Bob = [2,2,4,3,5]\n",
    "\n",
    "pearsonr(Alice, Bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=-1.0, pvalue=0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eve = [5,5,3,4,2]\n",
    "pearsonr(Alice, eve)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. #### Cosine Similarity\n",
    "We used this extensively while building our [Content-based recommenders](https://github.com/CKawara/Recommendation-engines-with-Python/tree/main/Content-Based-Recommenders).\n",
    "\n",
    "It computes the cosine of the angle between two vectors in an n-dimensional space. When the score is 1, the vesctors are exactly similar. -1 in the other hand, denotes that the two vectors are exactly dissimilar to each other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Different similarity scores are appropriet in different scenarios, in cases where magnitudeis important, the Euclidean distance is important. But in our case, correlation is more important to us, so we will be using Pearson and cosine similarity scores for our engines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A type of unsupervised learning that groups data points into different classes in such a way that data points belong to a particular class are more similar to each other than data points belonging to different classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means clustering\n",
    "It takes in the data points and the number of clusters(k) as input, then it randomly plots k different points on the plane(centroids). it repeats assignment and reassignment of points to the centroids until there's no further change in the set of k centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
